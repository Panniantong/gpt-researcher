#!/usr/bin/env python3
"""
æµ‹è¯•é€†å‘APIä¸­ä½¿ç”¨çš„æ¨¡å‹æ˜¯å¦å¯ç”¨
Test script to check if models used in reverse API are available

è¿™ä¸ªè„šæœ¬ä¼šæµ‹è¯•ï¼š
1. LLMæ¨¡å‹ï¼ˆé€šè¿‡é€†å‘APIï¼‰ï¼šgpt-4o-mini, gpt-4.1, o4-mini
2. Embeddingæ¨¡å‹ï¼ˆé€šè¿‡å®˜æ–¹OpenAI APIï¼‰ï¼štext-embedding-3-small
3. æœç´¢å¼•æ“ï¼šTavily API
"""

import os
import asyncio
import json
from typing import Dict, Any, List
from datetime import datetime
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥ä¾¿å¯¼å…¥æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from gpt_researcher.config.config import Config
from gpt_researcher.llm_provider.generic.base import GenericLLMProvider
from gpt_researcher.memory.embeddings import get_embeddings_model


class ModelTester:
    """æ¨¡å‹å¯ç”¨æ€§æµ‹è¯•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.config = Config()
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "llm_models": {},
            "embedding_model": {},
            "search_engines": {},
            "summary": {
                "total_tests": 0,
                "passed": 0,
                "failed": 0
            }
        }
        
    def log_test(self, test_name: str, status: str, details: str = ""):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        print(f"[{status}] {test_name}")
        if details:
            print(f"    è¯¦æƒ…: {details}")
        
        self.results["summary"]["total_tests"] += 1
        if status == "PASS":
            self.results["summary"]["passed"] += 1
        else:
            self.results["summary"]["failed"] += 1
    
    async def test_llm_model(self, model_type: str, provider: str, model: str) -> bool:
        """æµ‹è¯•LLMæ¨¡å‹"""
        try:
            print(f"\nğŸ§  æµ‹è¯• {model_type} æ¨¡å‹: {provider}:{model}")
            
            # åˆ›å»ºLLMå®ä¾‹
            llm_provider = GenericLLMProvider.create_llm(
                provider=provider,
                model=model,
                temperature=0.1,
                max_tokens=100
            )
            
            # æµ‹è¯•ç®€å•å¯¹è¯
            test_prompt = "è¯·å›ç­”ï¼š1+1ç­‰äºå‡ ï¼Ÿåªéœ€è¦å›ç­”æ•°å­—ã€‚"
            response = await llm_provider.get_chat_response(test_prompt)
            
            if response and len(response.strip()) > 0:
                self.log_test(f"{model_type} ({provider}:{model})", "PASS", f"å“åº”: {response[:50]}...")
                self.results["llm_models"][model_type] = {
                    "provider": provider,
                    "model": model,
                    "status": "available",
                    "response_sample": response[:100]
                }
                return True
            else:
                self.log_test(f"{model_type} ({provider}:{model})", "FAIL", "æ— å“åº”")
                self.results["llm_models"][model_type] = {
                    "provider": provider,
                    "model": model,
                    "status": "no_response",
                    "error": "Empty response"
                }
                return False
                
        except Exception as e:
            error_msg = str(e)
            self.log_test(f"{model_type} ({provider}:{model})", "FAIL", error_msg)
            self.results["llm_models"][model_type] = {
                "provider": provider,
                "model": model,
                "status": "error",
                "error": error_msg
            }
            return False
    
    async def test_embedding_model(self) -> bool:
        """æµ‹è¯•Embeddingæ¨¡å‹"""
        try:
            print(f"\nğŸ” æµ‹è¯• Embedding æ¨¡å‹: {self.config.embedding}")
            
            # è·å–embeddingæ¨¡å‹
            embeddings = get_embeddings_model(
                provider=self.config.embedding_provider,
                model=self.config.embedding_model,
                config=self.config
            )
            
            # æµ‹è¯•embedding
            test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
            embedding_result = await embeddings.aembed_query(test_text)
            
            if embedding_result and len(embedding_result) > 0:
                self.log_test(f"Embedding ({self.config.embedding})", "PASS", 
                            f"å‘é‡ç»´åº¦: {len(embedding_result)}")
                self.results["embedding_model"] = {
                    "provider": self.config.embedding_provider,
                    "model": self.config.embedding_model,
                    "status": "available",
                    "dimension": len(embedding_result)
                }
                return True
            else:
                self.log_test(f"Embedding ({self.config.embedding})", "FAIL", "æ— æ³•ç”Ÿæˆå‘é‡")
                self.results["embedding_model"] = {
                    "provider": self.config.embedding_provider,
                    "model": self.config.embedding_model,
                    "status": "no_embedding",
                    "error": "Empty embedding result"
                }
                return False
                
        except Exception as e:
            error_msg = str(e)
            self.log_test(f"Embedding ({self.config.embedding})", "FAIL", error_msg)
            self.results["embedding_model"] = {
                "provider": self.config.embedding_provider,
                "model": self.config.embedding_model,
                "status": "error",
                "error": error_msg
            }
            return False
    
    async def test_tavily_search(self) -> bool:
        """æµ‹è¯•Tavilyæœç´¢å¼•æ“"""
        try:
            print(f"\nğŸ” æµ‹è¯• Tavily æœç´¢å¼•æ“")
            
            # å¯¼å…¥Tavilyæœç´¢å™¨
            from gpt_researcher.retrievers.tavily.tavily import TavilySearch
            
            # åˆ›å»ºæœç´¢å®ä¾‹
            tavily = TavilySearch()
            
            # æµ‹è¯•æœç´¢
            test_query = "Python programming"
            search_results = await tavily.search(test_query, max_results=2)
            
            if search_results and len(search_results) > 0:
                self.log_test("Tavily Search", "PASS", f"è¿”å› {len(search_results)} ä¸ªç»“æœ")
                self.results["search_engines"]["tavily"] = {
                    "status": "available",
                    "results_count": len(search_results)
                }
                return True
            else:
                self.log_test("Tavily Search", "FAIL", "æ— æœç´¢ç»“æœ")
                self.results["search_engines"]["tavily"] = {
                    "status": "no_results",
                    "error": "Empty search results"
                }
                return False
                
        except Exception as e:
            error_msg = str(e)
            self.log_test("Tavily Search", "FAIL", error_msg)
            self.results["search_engines"]["tavily"] = {
                "status": "error",
                "error": error_msg
            }
            return False
    
    def print_environment_info(self):
        """æ‰“å°ç¯å¢ƒä¿¡æ¯"""
        print("=" * 60)
        print("ğŸ”§ ç¯å¢ƒé…ç½®ä¿¡æ¯")
        print("=" * 60)
        print(f"OPENAI_API_KEY: {'âœ“ å·²è®¾ç½®' if os.getenv('OPENAI_API_KEY') else 'âœ— æœªè®¾ç½®'}")
        print(f"OPENAI_BASE_URL: {os.getenv('OPENAI_BASE_URL', 'æœªè®¾ç½®')}")
        print(f"TAVILY_API_KEY: {'âœ“ å·²è®¾ç½®' if os.getenv('TAVILY_API_KEY') else 'âœ— æœªè®¾ç½®'}")
        print(f"EMBEDDING: {self.config.embedding}")
        print(f"FAST_LLM: {self.config.fast_llm}")
        print(f"SMART_LLM: {self.config.smart_llm}")
        print(f"STRATEGIC_LLM: {self.config.strategic_llm}")
        print()
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æµ‹è¯•æ€»ç»“")
        print("=" * 60)
        summary = self.results["summary"]
        print(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"é€šè¿‡: {summary['passed']} âœ“")
        print(f"å¤±è´¥: {summary['failed']} âœ—")
        print(f"æˆåŠŸç‡: {(summary['passed']/summary['total_tests']*100):.1f}%")
        
        if summary['failed'] > 0:
            print("\nâš ï¸  å¤±è´¥çš„æµ‹è¯•:")
            for category, tests in self.results.items():
                if category in ["llm_models", "embedding_model", "search_engines"]:
                    if isinstance(tests, dict):
                        for test_name, result in tests.items():
                            if result.get("status") != "available":
                                print(f"  - {test_name}: {result.get('error', 'Unknown error')}")
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ¨¡å‹å¯ç”¨æ€§æµ‹è¯•")
        self.print_environment_info()
        
        # æµ‹è¯•LLMæ¨¡å‹
        await self.test_llm_model("FAST_LLM", self.config.fast_llm_provider, self.config.fast_llm_model)
        await self.test_llm_model("SMART_LLM", self.config.smart_llm_provider, self.config.smart_llm_model)
        await self.test_llm_model("STRATEGIC_LLM", self.config.strategic_llm_provider, self.config.strategic_llm_model)
        
        # æµ‹è¯•Embeddingæ¨¡å‹
        await self.test_embedding_model()
        
        # æµ‹è¯•æœç´¢å¼•æ“
        await self.test_tavily_search()
        
        # æ‰“å°æ€»ç»“
        self.print_summary()
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        with open("model_test_results.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: model_test_results.json")


async def main():
    """ä¸»å‡½æ•°"""
    tester = ModelTester()
    await tester.run_all_tests()


if __name__ == "__main__":
    asyncio.run(main())
