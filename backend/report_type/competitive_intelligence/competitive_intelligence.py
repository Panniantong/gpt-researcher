from fastapi import WebSocket
from typing import Any, List, Optional, Dict
from urllib.parse import urlparse

from gpt_researcher import GPTResearcher


class CompetitiveIntelligenceReport:
    def __init__(
        self,
        query: str,
        query_domains: list = [],
        report_type: str = "competitive_intelligence",
        report_source: str = "web",
        source_urls: List[str] = [],
        document_urls: List[str] = [],
        tone: Any = "",
        config_path: str = None,
        websocket: WebSocket = None,
        headers: Optional[Dict] = None,
        mcp_configs=None,
        mcp_strategy=None,
    ):
        """
        Initialize Competitive Intelligence Report
        
        Args:
            query (str): Product name or URL to research
            query_domains (list): Domains to search within
            report_type (str): Type of competitive intelligence report
            report_source (str): Source of research data
            source_urls (List[str]): Specific URLs to include
            document_urls (List[str]): Document URLs to analyze
            tone: Writing tone for the report
            config_path (str): Path to configuration file
            websocket (WebSocket): WebSocket connection for real-time updates
            headers (Optional[Dict]): HTTP headers
            mcp_configs: MCP configurations
            mcp_strategy: MCP strategy
        """
        self.query = query
        self.query_domains = query_domains
        self.report_type = report_type
        self.report_source = report_source
        self.source_urls = source_urls
        self.document_urls = document_urls
        self.tone = tone
        self.config_path = config_path
        self.websocket = websocket
        self.headers = headers or {}

        # ä¸ºç«å“è°ƒç ”é…ç½®å¤šæœç´¢å¼•æ“ï¼štavily + google
        # è¿™æ ·å¯ä»¥è·å¾—æ›´å…¨é¢çš„æœç´¢è¦†ç›–ï¼Œç‰¹åˆ«æ˜¯Googleå¯¹æŸäº›å¹³å°çš„ç´¢å¼•å¯èƒ½æ›´å¥½
        competitive_headers = self.headers.copy()
        if "retrievers" not in competitive_headers:
            competitive_headers["retrievers"] = "tavily,google"

        # Initialize researcher with optional MCP parameters
        gpt_researcher_params = {
            "query": self.query,
            "query_domains": self.query_domains,
            "report_type": self.report_type,
            "report_source": self.report_source,
            "source_urls": self.source_urls,
            "document_urls": self.document_urls,
            "tone": self.tone,
            "config_path": self.config_path,
            "websocket": self.websocket,
            "headers": competitive_headers,  # ä½¿ç”¨åŒ…å«å¤šæœç´¢å¼•æ“é…ç½®çš„headers
        }
        
        # Add MCP parameters if provided
        if mcp_configs is not None:
            gpt_researcher_params["mcp_configs"] = mcp_configs
        if mcp_strategy is not None:
            gpt_researcher_params["mcp_strategy"] = mcp_strategy
            
        self.gpt_researcher = GPTResearcher(**gpt_researcher_params)

        # å®šä¹‰å…³é”®å¹³å°ä¼˜å…ˆçº§ï¼ˆç”¨äºç»“æœè¿‡æ»¤ä¼˜åŒ–ï¼‰
        # é’ˆå¯¹tavily+googleåŒæœç´¢å¼•æ“ï¼Œæ‰©å±•å¹³å°è¦†ç›–èŒƒå›´
        self.priority_platforms = [
            # æ ¸å¿ƒåˆ›å§‹äººå’Œå›¢é˜Ÿä¿¡æ¯å¹³å°
            "linkedin.com",
            "crunchbase.com",
            "angel.co",
            "angellist.com",

            # ç”¨æˆ·åé¦ˆå’Œç¤¾åŒºè®¨è®ºå¹³å°
            "reddit.com",
            "producthunt.com",
            "news.ycombinator.com",  # Hacker News
            "hackernews.com",

            # åˆ›ä¸šå’Œå¢é•¿æ•…äº‹å¹³å°
            "indiehackers.com",
            "medium.com",
            "substack.com",

            # æŠ€æœ¯å’Œå¼€å‘å¹³å°
            "github.com",
            "stackoverflow.com",
            "dev.to",

            # ä¸“ä¸šè¯„ä»·å’Œæ¯”è¾ƒå¹³å°
            "g2.com",
            "capterra.com",
            "trustpilot.com",
            "getapp.com",

            # æ–°é—»å’Œåª’ä½“å¹³å°
            "techcrunch.com",
            "venturebeat.com",
            "theverge.com",
            "wired.com",

            # è§†é¢‘å’Œæ¼”ç¤ºå¹³å°
            "youtube.com",
            "vimeo.com",

            # å…¶ä»–æœ‰ä»·å€¼çš„å¹³å°
            "twitter.com",
            "x.com",
            "facebook.com",
            "blog.com",
            "wordpress.com"
        ]

    async def run(self):
        """
        Execute competitive intelligence research and generate report with platform optimization

        Returns:
            str: The generated competitive intelligence report
        """
        # Conduct research phase
        await self.gpt_researcher.conduct_research()

        # ç¬¬2æ­¥ï¼šç»“æœè¿‡æ»¤ä¼˜åŒ– - é‡æ–°æ’åºç ”ç©¶ä¸Šä¸‹æ–‡ï¼Œä¼˜å…ˆå±•ç¤ºå…³é”®å¹³å°ä¿¡æ¯
        if hasattr(self.gpt_researcher, 'context') and self.gpt_researcher.context:
            self.gpt_researcher.context = self._prioritize_platform_results(self.gpt_researcher.context)

        # ç¬¬3æ­¥ï¼šä¸Šä¸‹æ–‡å¢å¼º - æ·»åŠ å¹³å°æŒ‡å¯¼ä¿¡æ¯
        if hasattr(self.gpt_researcher, 'context') and self.gpt_researcher.context:
            enhanced_context = self._enhance_context_with_platform_guidance(self.gpt_researcher.context)
            self.gpt_researcher.context = enhanced_context

        # Generate competitive intelligence report
        report = await self.gpt_researcher.write_report()

        return report

    def _process_product_input(self, query: str) -> str:
        """
        Process product input (name or URL) and prepare for research
        
        Args:
            query (str): Product name or URL
            
        Returns:
            str: Processed query for research
        """
        # If query looks like a URL, extract additional context
        if query.startswith(("http://", "https://", "www.")):
            # For URL inputs, we might want to add the URL to source_urls
            if query not in self.source_urls:
                self.source_urls.append(query)
            
            # Extract product name from URL if possible
            try:
                from urllib.parse import urlparse
                parsed = urlparse(query)
                domain = parsed.netloc.replace("www.", "")
                product_name = domain.split(".")[0].title()
                return f"{product_name} (product intelligence research for {query})"
            except:
                return f"Product intelligence research for {query}"
        else:
            # For product names, enhance the query for better research
            return f"{query} (competitive intelligence analysis)"

    def _enhance_query_for_competitive_research(self, query: str) -> str:
        """
        Enhance query specifically for competitive intelligence research with platform-optimized queries

        Args:
            query (str): Original query

        Returns:
            str: Enhanced query with competitive intelligence focus and platform guidance
        """
        # ç”Ÿæˆå¹³å°ä¼˜åŒ–çš„æŸ¥è¯¢ï¼Œå¼•å¯¼æœç´¢å¼•æ“è¿”å›å…³é”®å¹³å°çš„ç»“æœ
        platform_optimized_query = self._generate_platform_optimized_query(query)

        return platform_optimized_query

    def _generate_platform_optimized_query(self, product_name: str) -> str:
        """
        ç”Ÿæˆå¹³å°ä¼˜åŒ–çš„æŸ¥è¯¢ï¼Œé’ˆå¯¹tavily+googleåŒæœç´¢å¼•æ“ä¼˜åŒ–

        Args:
            product_name (str): äº§å“åç§°

        Returns:
            str: ä¼˜åŒ–åçš„æŸ¥è¯¢ï¼ŒåŒæ—¶é€‚é…Tavilyå’ŒGoogleæœç´¢ç‰¹æ€§
        """
        # æå–çº¯äº§å“åï¼ˆå»é™¤URLç­‰ï¼‰
        clean_product_name = self._extract_clean_product_name(product_name)

        # é’ˆå¯¹åŒæœç´¢å¼•æ“çš„ä¼˜åŒ–ç­–ç•¥ï¼š
        # 1. Tavily: ä½¿ç”¨å…³é”®è¯å¼•å¯¼ï¼Œæé«˜ç›¸å…³å¹³å°å†…å®¹è¿”å›æ¦‚ç‡
        # 2. Google: æ”¯æŒsite:è¯­æ³•ï¼Œå¯ä»¥æ›´ç²¾ç¡®åœ°æœç´¢ç‰¹å®šå¹³å°

        # åŸºç¡€æŸ¥è¯¢ï¼šäº§å“å + ç«å“åˆ†æ
        base_query = f"{clean_product_name} competitive intelligence analysis"

        # å¹³å°å…³é”®è¯ï¼ˆé€‚ç”¨äºTavilyå’ŒGoogleçš„é€šç”¨æœç´¢ï¼‰
        platform_keywords = [
            "LinkedIn founder CEO team background",  # åˆ›å§‹äººä¿¡æ¯
            "Reddit review discussion user experience",  # ç”¨æˆ·åé¦ˆ
            "ProductHunt launch feedback community",  # äº§å“è¯„ä»·
            "IndieHackers growth story revenue model",  # å¢é•¿æ•…äº‹
            "Crunchbase funding investment valuation",  # æŠ•èµ„ä¿¡æ¯
            "GitHub repository code open source",  # æŠ€æœ¯å®ç°
            "startup company business model strategy"  # å•†ä¸šæ¨¡å¼
        ]

        # ç»„åˆä¼˜åŒ–æŸ¥è¯¢
        # è¿™ä¸ªæŸ¥è¯¢æ—¢èƒ½è®©Tavilyç†è§£æˆ‘ä»¬æƒ³è¦çš„å†…å®¹ç±»å‹ï¼Œ
        # ä¹Ÿèƒ½è®©Googleé€šè¿‡å…³é”®è¯åŒ¹é…æ‰¾åˆ°ç›¸å…³å¹³å°çš„å†…å®¹
        optimized_query = f"{base_query} {' '.join(platform_keywords)}"

        return optimized_query

    def _extract_clean_product_name(self, query: str) -> str:
        """
        ä»æŸ¥è¯¢ä¸­æå–å¹²å‡€çš„äº§å“åç§°

        Args:
            query (str): åŸå§‹æŸ¥è¯¢

        Returns:
            str: æ¸…ç†åçš„äº§å“åç§°
        """
        # å¦‚æœæ˜¯URLï¼Œæå–åŸŸåä½œä¸ºäº§å“å
        if query.startswith(("http://", "https://", "www.")):
            try:
                parsed = urlparse(query if query.startswith("http") else f"https://{query}")
                domain = parsed.netloc.replace("www.", "")
                product_name = domain.split(".")[0].title()
                return product_name
            except:
                return query

        # ç§»é™¤å¸¸è§çš„åç¼€è¯
        clean_name = query.replace("(competitive intelligence analysis)", "").strip()
        clean_name = clean_name.replace("(product intelligence research for", "").strip()
        clean_name = clean_name.replace(")", "").strip()

        return clean_name

    def _prioritize_platform_results(self, context_data):
        """
        ç¬¬2æ­¥ï¼šç»“æœè¿‡æ»¤ä¼˜åŒ– - å¯¹ç ”ç©¶ä¸Šä¸‹æ–‡æŒ‰å¹³å°é‡è¦æ€§é‡æ–°æ’åº

        Args:
            context_data: ç ”ç©¶ä¸Šä¸‹æ–‡æ•°æ®

        Returns:
            é‡æ–°æ’åºåçš„ä¸Šä¸‹æ–‡æ•°æ®ï¼Œå…³é”®å¹³å°ä¿¡æ¯ä¼˜å…ˆ
        """
        if not context_data:
            return context_data

        # å¦‚æœcontextæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›ï¼ˆæŸäº›æƒ…å†µä¸‹contextå¯èƒ½æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼‰
        if isinstance(context_data, str):
            return context_data

        # å¦‚æœcontextæ˜¯åˆ—è¡¨ï¼ŒæŒ‰å¹³å°ä¼˜å…ˆçº§æ’åº
        if isinstance(context_data, list):
            priority_items = []
            other_items = []

            for item in context_data:
                # æ£€æŸ¥itemæ˜¯å¦åŒ…å«URLä¿¡æ¯
                item_text = str(item)
                is_priority = False

                # æ£€æŸ¥æ˜¯å¦æ¥è‡ªå…³é”®å¹³å°
                for platform in self.priority_platforms:
                    if platform in item_text.lower():
                        priority_items.append(item)
                        is_priority = True
                        break

                if not is_priority:
                    other_items.append(item)

            # è¿”å›é‡æ–°æ’åºçš„ç»“æœï¼šå…³é”®å¹³å°ä¿¡æ¯åœ¨å‰
            return priority_items + other_items

        # å…¶ä»–æƒ…å†µç›´æ¥è¿”å›åŸæ•°æ®
        return context_data

    def _enhance_context_with_platform_guidance(self, context_data):
        """
        ç¬¬3æ­¥ï¼šä¸Šä¸‹æ–‡å¢å¼º - åœ¨ç ”ç©¶ä¸Šä¸‹æ–‡ä¸­æ·»åŠ å¹³å°åˆ†ææŒ‡å¯¼

        Args:
            context_data: åŸå§‹ç ”ç©¶ä¸Šä¸‹æ–‡

        Returns:
            å¢å¼ºåçš„ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«å¹³å°åˆ†ææŒ‡å¯¼
        """
        if not context_data:
            return context_data

        # å¹³å°åˆ†ææŒ‡å¯¼æ–‡æœ¬ï¼ˆé’ˆå¯¹tavily+googleåŒæœç´¢å¼•æ“ä¼˜åŒ–ï¼‰
        platform_guidance = """

=== å¤šæœç´¢å¼•æ“å¹³å°ä¿¡æ¯åˆ†ææŒ‡å¯¼ ===

æœ¬æ¬¡ç ”ç©¶ä½¿ç”¨äº†Tavily + GoogleåŒæœç´¢å¼•æ“ï¼Œè¯·ç‰¹åˆ«å…³æ³¨å¹¶ä¼˜å…ˆåˆ†ææ¥è‡ªä»¥ä¸‹å…³é”®å¹³å°çš„ä¿¡æ¯ï¼š

ğŸ” **åˆ›å§‹äººå’Œå›¢é˜Ÿä¿¡æ¯**ï¼š
- LinkedIn: åˆ›å§‹äººèƒŒæ™¯ã€å·¥ä½œç»å†ã€æ•™è‚²èƒŒæ™¯ã€å›¢é˜Ÿæ„æˆã€èŒä¸šç½‘ç»œ
- Crunchbase: å…¬å¸å›¢é˜Ÿä¿¡æ¯ã€æŠ•èµ„äººå…³ç³»ã€é¡¾é—®å›¢é˜Ÿã€ç®¡ç†å±‚å˜åŠ¨
- AngelList: æ—©æœŸå›¢é˜Ÿæ„æˆã€è‚¡æƒåˆ†é…ã€æ‹›è˜ä¿¡æ¯

ğŸ‘¥ **ç”¨æˆ·åé¦ˆå’Œå¸‚åœºè®¤çŸ¥**ï¼š
- Reddit: çœŸå®ç”¨æˆ·è®¨è®ºã€ä½¿ç”¨ä½“éªŒã€äº§å“å¯¹æ¯”ã€é—®é¢˜åé¦ˆã€ç¤¾åŒºå£ç¢‘
- Product Hunt: äº§å“å‘å¸ƒåé¦ˆã€ç¤¾åŒºè¯„ä»·ã€åŠŸèƒ½è®¨è®ºã€ç«å“å¯¹æ¯”
- Hacker News: æŠ€æœ¯ç¤¾åŒºè®¨è®ºã€å¼€å‘è€…è§‚ç‚¹ã€è¡Œä¸šè¶‹åŠ¿

ğŸ“ˆ **å¢é•¿å’Œå•†ä¸šæ¨¡å¼**ï¼š
- Indie Hackers: å¢é•¿æ•…äº‹ã€æ”¶å…¥æ•°æ®ã€è¥é”€ç­–ç•¥ã€åˆ›ä¸šç»éªŒåˆ†äº«
- Medium/Substack: åˆ›å§‹äººåˆ†äº«ã€å¢é•¿å¤ç›˜ã€è¡Œä¸šæ´å¯Ÿã€æˆ˜ç•¥æ€è€ƒ

ğŸ’» **æŠ€æœ¯å®ç°**ï¼š
- GitHub: å¼€æºä»£ç ã€æŠ€æœ¯æ ˆã€æ¶æ„è®¾è®¡ã€å¼€å‘æ´»è·ƒåº¦ã€è´¡çŒ®è€…
- Stack Overflow: æŠ€æœ¯é—®é¢˜ã€å®ç°éš¾ç‚¹ã€å¼€å‘è€…è®¨è®º
- Dev.to: æŠ€æœ¯åšå®¢ã€å¼€å‘ç»éªŒã€æ¶æ„åˆ†äº«

ğŸ’° **æŠ•èµ„å’Œä¼°å€¼**ï¼š
- Crunchbase: èèµ„å†å²ã€æŠ•èµ„è½®æ¬¡ã€ä¼°å€¼ä¿¡æ¯ã€æŠ•èµ„äººèƒŒæ™¯
- AngelList: æ—©æœŸæŠ•èµ„ã€è‚¡æƒä¿¡æ¯ã€æŠ•èµ„æ¡ä»¶

ğŸ“Š **ä¸“ä¸šè¯„ä»·å’Œæ¯”è¾ƒ**ï¼š
- G2/Capterra/GetApp: ä¸“ä¸šç”¨æˆ·è¯„åˆ†ã€åŠŸèƒ½å¯¹æ¯”ã€ç«å“åˆ†æ
- Trustpilot: ç”¨æˆ·æ»¡æ„åº¦ã€æœåŠ¡è´¨é‡è¯„ä»·

ğŸ“° **åª’ä½“æŠ¥é“å’Œè¡Œä¸šåˆ†æ**ï¼š
- TechCrunch/VentureBeat: è¡Œä¸šæ–°é—»ã€èèµ„æŠ¥é“ã€äº§å“å‘å¸ƒ
- The Verge/Wired: æ·±åº¦åˆ†æã€è¡Œä¸šè¶‹åŠ¿ã€æŠ€æœ¯è¯„æµ‹

ğŸ¥ **äº§å“æ¼”ç¤ºå’Œæ•™ç¨‹**ï¼š
- YouTube: äº§å“æ¼”ç¤ºã€ç”¨æˆ·æ•™ç¨‹ã€è¯„æµ‹è§†é¢‘ã€åˆ›å§‹äººè®¿è°ˆ

**åŒæœç´¢å¼•æ“ä¼˜åŠ¿åˆ†æ**ï¼š
1. **Tavilyä¼˜åŠ¿**: å®æ—¶ä¿¡æ¯ã€AIä¼˜åŒ–çš„å†…å®¹æå–ã€ç›¸å…³æ€§æ’åº
2. **Googleä¼˜åŠ¿**: å…¨é¢çš„ç´¢å¼•è¦†ç›–ã€ç²¾ç¡®çš„site:æœç´¢ã€å†å²ä¿¡æ¯

**åˆ†æè¦æ±‚**ï¼š
1. ä¼˜å…ˆåˆ†ææ¥è‡ªå…³é”®å¹³å°çš„ä¿¡æ¯ï¼Œå¹¶æ˜ç¡®æ ‡æ³¨ä¿¡æ¯æ¥æºå’Œæœç´¢å¼•æ“
2. å¯¹æ¯”ä¸åŒå¹³å°çš„ä¿¡æ¯ï¼Œè¯†åˆ«ä¸€è‡´æ€§å’Œå·®å¼‚æ€§
3. é‡ç‚¹å…³æ³¨æ—¶æ•ˆæ€§ï¼šæ ‡æ³¨ä¿¡æ¯è·å–æ—¶é—´ï¼ŒåŒºåˆ†æœ€æ–°åŠ¨æ€å’Œå†å²ä¿¡æ¯
4. å¦‚æœæŸä¸ªé‡è¦ç»´åº¦ç¼ºå°‘å…³é”®å¹³å°ä¿¡æ¯ï¼Œè¯·æ ‡æ³¨"âš ï¸ ä¿¡æ¯ä¸è¶³ï¼Œå»ºè®®è¡¥å……è°ƒç ”"

"""

        # æ ¹æ®context_dataçš„ç±»å‹è¿›è¡Œä¸åŒå¤„ç†
        if isinstance(context_data, str):
            return context_data + platform_guidance
        elif isinstance(context_data, list):
            # å°†æŒ‡å¯¼ä¿¡æ¯ä½œä¸ºç¬¬ä¸€ä¸ªå…ƒç´ æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            enhanced_context = [platform_guidance] + context_data
            return enhanced_context
        else:
            # å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²åæ·»åŠ æŒ‡å¯¼
            return str(context_data) + platform_guidance


class CompetitiveIntelligenceDetailedReport(CompetitiveIntelligenceReport):
    """
    Detailed version of Competitive Intelligence Report with more comprehensive analysis
    """
    
    def __init__(self, *args, **kwargs):
        # Set default report type to detailed version
        kwargs["report_type"] = kwargs.get("report_type", "competitive_intelligence_detailed")
        super().__init__(*args, **kwargs)

    async def run(self):
        """
        Execute detailed competitive intelligence research and generate comprehensive report with platform optimization

        Returns:
            str: The generated detailed competitive intelligence report
        """
        # Enhance query for detailed research
        original_query = self.gpt_researcher.query
        self.gpt_researcher.query = self._enhance_query_for_detailed_research(original_query)

        # Conduct more thorough research phase
        await self.gpt_researcher.conduct_research()

        # ç¬¬2æ­¥ï¼šç»“æœè¿‡æ»¤ä¼˜åŒ– - é‡æ–°æ’åºç ”ç©¶ä¸Šä¸‹æ–‡ï¼Œä¼˜å…ˆå±•ç¤ºå…³é”®å¹³å°ä¿¡æ¯
        if hasattr(self.gpt_researcher, 'context') and self.gpt_researcher.context:
            self.gpt_researcher.context = self._prioritize_platform_results(self.gpt_researcher.context)

        # ç¬¬3æ­¥ï¼šä¸Šä¸‹æ–‡å¢å¼º - æ·»åŠ å¹³å°æŒ‡å¯¼ä¿¡æ¯ï¼ˆè¯¦ç»†æ¨¡å¼ä½¿ç”¨æ›´å…¨é¢çš„æŒ‡å¯¼ï¼‰
        if hasattr(self.gpt_researcher, 'context') and self.gpt_researcher.context:
            enhanced_context = self._enhance_context_with_detailed_platform_guidance(self.gpt_researcher.context)
            self.gpt_researcher.context = enhanced_context

        # Generate detailed competitive intelligence report
        report = await self.gpt_researcher.write_report()

        return report

    def _enhance_query_for_detailed_research(self, query: str) -> str:
        """
        Enhance query for detailed competitive intelligence research with deeper platform coverage

        Args:
            query (str): Original query

        Returns:
            str: Enhanced query for detailed analysis with expanded platform keywords
        """
        # è·å–åŸºç¡€çš„å¹³å°ä¼˜åŒ–æŸ¥è¯¢
        base_query = self._generate_platform_optimized_query(query)

        # ä¸ºè¯¦ç»†æ¨¡å¼æ·»åŠ æ›´å¤šæ·±åº¦åˆ†æå…³é”®è¯
        detailed_keywords = [
            "financial metrics revenue model",
            "user acquisition growth strategy",
            "technology stack architecture",
            "competitive landscape market analysis",
            "investment funding valuation",
            "team background experience",
            "user feedback testimonials",
            "pricing strategy business model"
        ]

        # ç»„åˆè¯¦ç»†æŸ¥è¯¢
        detailed_query = f"{base_query} {' '.join(detailed_keywords)} deep analysis comprehensive research"

        return detailed_query

    def _enhance_context_with_detailed_platform_guidance(self, context_data):
        """
        ä¸ºè¯¦ç»†æ¨¡å¼æä¾›æ›´å…¨é¢çš„å¹³å°åˆ†ææŒ‡å¯¼

        Args:
            context_data: åŸå§‹ç ”ç©¶ä¸Šä¸‹æ–‡

        Returns:
            å¢å¼ºåçš„ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«è¯¦ç»†çš„å¹³å°åˆ†ææŒ‡å¯¼
        """
        if not context_data:
            return context_data

        # è¯¦ç»†æ¨¡å¼çš„å¹³å°åˆ†ææŒ‡å¯¼æ–‡æœ¬
        detailed_platform_guidance = """

=== è¯¦ç»†ç«å“æƒ…æŠ¥åˆ†ææŒ‡å¯¼ ===

åœ¨è¿›è¡Œæ·±åº¦ç«å“åˆ†ææ—¶ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ¡†æ¶ç³»ç»Ÿæ€§åœ°åˆ†æå„å¹³å°ä¿¡æ¯ï¼š

ğŸ¢ **åŸºç¡€ä¿¡æ¯æ”¶é›†**ï¼š
- LinkedIn: åˆ›å§‹äººå®Œæ•´èƒŒæ™¯ã€å›¢é˜Ÿè§„æ¨¡ã€å…³é”®å‘˜å·¥ã€å…¬å¸å‘å±•å†ç¨‹
- Crunchbase: æˆç«‹æ—¶é—´ã€æ€»éƒ¨ä½ç½®ã€å‘˜å·¥æ•°é‡ã€ä¸šåŠ¡æ¨¡å¼ã€æŠ•èµ„çŠ¶æ€

ğŸ‘¨â€ğŸ’¼ **åˆ›å§‹äºº/å›¢é˜Ÿæ·±åº¦åˆ†æ**ï¼š
- LinkedIn: æ•™è‚²èƒŒæ™¯ã€å·¥ä½œç»å†ã€è¡Œä¸šç»éªŒã€é¢†å¯¼é£æ ¼ã€ç½‘ç»œå…³ç³»
- Medium/ä¸ªäººåšå®¢: åˆ›å§‹äººæ€è€ƒã€ä»·å€¼è§‚ã€æˆ˜ç•¥è§‚ç‚¹
- Twitter: è¡Œä¸šå½±å“åŠ›ã€è§‚ç‚¹è¡¨è¾¾ã€ç¤¾äº¤ç½‘ç»œ

ğŸ“Š **å…«ç»´å•†ä¸šåˆ†æ**ï¼š
1. å¸‚åœºå®šä½: Reddit/HackerNewsç”¨æˆ·è®¨è®ºã€G2/Capterraä¸“ä¸šè¯„ä»·
2. äº§å“åŠŸèƒ½: ProductHuntåŠŸèƒ½ä»‹ç»ã€GitHubæŠ€æœ¯å®ç°
3. ç”¨æˆ·ä½“éªŒ: RedditçœŸå®åé¦ˆã€App Store/Google Playè¯„ä»·
4. å•†ä¸šæ¨¡å¼: IndieHackersæ”¶å…¥åˆ†äº«ã€å…¬å¸åšå®¢å•†ä¸šç­–ç•¥
5. æŠ€æœ¯æ¶æ„: GitHubä»£ç åˆ†æã€æŠ€æœ¯åšå®¢æ¶æ„åˆ†äº«
6. å›¢é˜Ÿèƒ½åŠ›: LinkedInå›¢é˜ŸèƒŒæ™¯ã€Crunchbaseå›¢é˜Ÿä¿¡æ¯
7. èµ„é‡‘çŠ¶å†µ: Crunchbaseèèµ„å†å²ã€æ–°é—»æŠ¥é“
8. å¢é•¿ç­–ç•¥: IndieHackerså¢é•¿æ•…äº‹ã€è¥é”€æ¡ˆä¾‹åˆ†æ

ğŸ“ˆ **è¥é”€æƒ…æŠ¥æ·±åº¦æŒ–æ˜**ï¼š
- IndieHackers: å¢é•¿æ—¶é—´çº¿ã€ç”¨æˆ·è·å–ç­–ç•¥ã€æ”¶å…¥é‡Œç¨‹ç¢‘
- ProductHunt: å‘å¸ƒç­–ç•¥ã€ç¤¾åŒºååº”ã€ä¼ æ’­æ•ˆæœ
- Reddit: ç”¨æˆ·è‡ªå‘è®¨è®ºã€å£ç¢‘ä¼ æ’­ã€ç—…æ¯’å¼å¢é•¿
- ç¤¾äº¤åª’ä½“: å†…å®¹è¥é”€ã€ç¤¾åŒºå»ºè®¾ã€å“ç‰Œä¼ æ’­

ğŸ”§ **å¤åˆ»å¯è¡Œæ€§è¯„ä¼°**ï¼š
- GitHub: æŠ€æœ¯å¤æ‚åº¦ã€å¼€æºç¨‹åº¦ã€æŠ€æœ¯æ ˆåˆ†æ
- æŠ€æœ¯åšå®¢: æ¶æ„è®¾è®¡ã€æŠ€æœ¯é€‰å‹ã€å¼€å‘éš¾ç‚¹
- æ‹›è˜ä¿¡æ¯: æŠ€æœ¯è¦æ±‚ã€å›¢é˜Ÿè§„æ¨¡ã€å¼€å‘å‘¨æœŸ

ğŸ’¡ **Executive Summaryè¦ç´ **ï¼š
- æ ¸å¿ƒç«äº‰ä¼˜åŠ¿è¯†åˆ«
- å¯å¤åˆ¶çš„å¢é•¿ç­–ç•¥
- æŠ€æœ¯å®ç°çš„å…³é”®è¦ç‚¹
- å¸‚åœºæœºä¼šå’Œå¨èƒåˆ†æ

**åˆ†ææ·±åº¦è¦æ±‚**ï¼š
1. **å®šé‡åˆ†æ**: å°½å¯èƒ½æ”¶é›†å…·ä½“æ•°å­—ï¼ˆç”¨æˆ·æ•°ã€æ”¶å…¥ã€èèµ„é¢ã€å›¢é˜Ÿè§„æ¨¡ç­‰ï¼‰
2. **å®šæ€§åˆ†æ**: æ·±åº¦è§£è¯»ç­–ç•¥æ€è·¯ã€æ‰§è¡Œç»†èŠ‚ã€æˆåŠŸå› ç´ 
3. **æ—¶é—´ç»´åº¦**: åˆ†æå‘å±•å†ç¨‹ã€å…³é”®èŠ‚ç‚¹ã€å¢é•¿è½¨è¿¹
4. **å¯¹æ¯”ç»´åº¦**: ä¸ç«å“å¯¹æ¯”ã€ä¸è¡Œä¸šæ ‡å‡†å¯¹æ¯”
5. **é£é™©è¯„ä¼°**: è¯†åˆ«æ½œåœ¨é£é™©ã€å¸‚åœºå¨èƒã€æŠ€æœ¯æŒ‘æˆ˜

**ä¿¡æ¯éªŒè¯è¦æ±‚**ï¼š
- å¤šæºéªŒè¯: åŒä¸€ä¿¡æ¯å°½é‡ä»å¤šä¸ªå¹³å°éªŒè¯
- æ—¶æ•ˆæ€§æ£€æŸ¥: æ ‡æ³¨ä¿¡æ¯è·å–æ—¶é—´ï¼Œè¯†åˆ«è¿‡æ—¶ä¿¡æ¯
- å¯ä¿¡åº¦è¯„ä¼°: åŒºåˆ†å®˜æ–¹ä¿¡æ¯ã€ç¬¬ä¸‰æ–¹è¯„ä»·ã€ç”¨æˆ·åé¦ˆ
- ç¼ºå¤±æ ‡æ³¨: æ˜ç¡®æ ‡æ³¨ä¿¡æ¯ä¸è¶³çš„é¢†åŸŸï¼Œå»ºè®®è¡¥å……è°ƒç ”æ–¹å‘

"""

        # æ ¹æ®context_dataçš„ç±»å‹è¿›è¡Œä¸åŒå¤„ç†
        if isinstance(context_data, str):
            return context_data + detailed_platform_guidance
        elif isinstance(context_data, list):
            # å°†è¯¦ç»†æŒ‡å¯¼ä¿¡æ¯ä½œä¸ºç¬¬ä¸€ä¸ªå…ƒç´ æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            enhanced_context = [detailed_platform_guidance] + context_data
            return enhanced_context
        else:
            # å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²åæ·»åŠ æŒ‡å¯¼
            return str(context_data) + detailed_platform_guidance