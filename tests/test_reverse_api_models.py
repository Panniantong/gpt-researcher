#!/usr/bin/env python3
"""
é€†å‘APIæ¨¡å‹æµ‹è¯•è„šæœ¬
æµ‹è¯•æ‰€æœ‰é…ç½®çš„æ¨¡å‹æ˜¯å¦å¯ç”¨ï¼ŒåŒ…æ‹¬LLMå’ŒEmbeddingæ¨¡å‹
"""

import asyncio
import os
import sys
import traceback
from typing import Dict, List, Tuple
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from gpt_researcher.config.config import Config
from gpt_researcher.utils.llm import create_chat_completion
from gpt_researcher.memory.embeddings import Memory

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class ModelTester:
    """æ¨¡å‹æµ‹è¯•å™¨ç±»"""
    
    def __init__(self):
        self.config = Config()
        self.results = {}
        
    def print_config_info(self):
        """æ‰“å°å½“å‰é…ç½®ä¿¡æ¯"""
        print("=" * 60)
        print("ğŸ”§ å½“å‰é…ç½®ä¿¡æ¯")
        print("=" * 60)
        
        # APIé…ç½®
        print("\nğŸ“¡ APIé…ç½®:")
        print(f"  OPENAI_API_KEY: {'âœ“ å·²è®¾ç½®' if os.getenv('OPENAI_API_KEY') else 'âœ— æœªè®¾ç½®'}")
        print(f"  OPENAI_BASE_URL: {os.getenv('OPENAI_BASE_URL', 'æœªè®¾ç½®')}")
        print(f"  OPENAI_EMBEDDING_API_KEY: {'âœ“ å·²è®¾ç½®' if os.getenv('OPENAI_EMBEDDING_API_KEY') else 'âœ— æœªè®¾ç½®'}")
        print(f"  OPENAI_EMBEDDING_BASE_URL: {os.getenv('OPENAI_EMBEDDING_BASE_URL', 'æœªè®¾ç½®')}")
        print(f"  TAVILY_API_KEY: {'âœ“ å·²è®¾ç½®' if os.getenv('TAVILY_API_KEY') else 'âœ— æœªè®¾ç½®'}")
        
        # LLMé…ç½®
        print("\nğŸ¤– LLMæ¨¡å‹é…ç½®:")
        print(f"  FAST_LLM: {self.config.fast_llm}")
        print(f"  SMART_LLM: {self.config.smart_llm}")
        print(f"  STRATEGIC_LLM: {self.config.strategic_llm}")
        print(f"  è§£æåçš„Fast LLM: {self.config.fast_llm_provider}:{self.config.fast_llm_model}")
        print(f"  è§£æåçš„Smart LLM: {self.config.smart_llm_provider}:{self.config.smart_llm_model}")
        print(f"  è§£æåçš„Strategic LLM: {self.config.strategic_llm_provider}:{self.config.strategic_llm_model}")
        
        # Embeddingé…ç½®
        print("\nğŸ” Embeddingé…ç½®:")
        print(f"  EMBEDDING: {self.config.embedding}")
        print(f"  è§£æåçš„Embedding: {self.config.embedding_provider}:{self.config.embedding_model}")
        
        # Tokené™åˆ¶
        print("\nğŸ“Š Tokené™åˆ¶:")
        print(f"  FAST_TOKEN_LIMIT: {self.config.fast_token_limit}")
        print(f"  SMART_TOKEN_LIMIT: {self.config.smart_token_limit}")
        print(f"  STRATEGIC_TOKEN_LIMIT: {self.config.strategic_token_limit}")
        
        print("\n" + "=" * 60)

    async def test_llm_model(self, model_name: str, provider: str, model: str, token_limit: int) -> Tuple[bool, str]:
        """æµ‹è¯•å•ä¸ªLLMæ¨¡å‹"""
        try:
            print(f"ğŸ§ª æµ‹è¯• {model_name} ({provider}:{model})...")
            
            # åˆ›å»ºæµ‹è¯•æ¶ˆæ¯
            test_messages = [
                {"role": "user", "content": "è¯·ç®€å•å›ç­”ï¼šä½ æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿç”¨ä¸€å¥è¯å›ç­”å³å¯ã€‚"}
            ]
            
            # è°ƒç”¨LLM
            response = await create_chat_completion(
                model=model,
                messages=test_messages,
                temperature=0.1,
                llm_provider=provider,
                stream=False,  # ä¸ä½¿ç”¨æµå¼è¾“å‡ºä»¥ä¾¿è·å–å®Œæ•´å“åº”
                max_tokens=100,  # é™åˆ¶tokenæ•°é‡
                llm_kwargs=self.config.llm_kwargs
            )
            
            if response and len(response.strip()) > 0:
                print(f"  âœ… {model_name} æµ‹è¯•æˆåŠŸ")
                print(f"  ğŸ“ å“åº”: {response[:100]}{'...' if len(response) > 100 else ''}")
                return True, response[:200]
            else:
                print(f"  âŒ {model_name} è¿”å›ç©ºå“åº”")
                return False, "ç©ºå“åº”"
                
        except Exception as e:
            error_msg = str(e)
            print(f"  âŒ {model_name} æµ‹è¯•å¤±è´¥: {error_msg}")
            return False, error_msg

    async def test_embedding_model(self) -> Tuple[bool, str]:
        """æµ‹è¯•Embeddingæ¨¡å‹"""
        try:
            print(f"ğŸ§ª æµ‹è¯• Embedding ({self.config.embedding_provider}:{self.config.embedding_model})...")
            
            # åˆ›å»ºMemoryå®ä¾‹
            memory = Memory(
                embedding_provider=self.config.embedding_provider,
                model=self.config.embedding_model
            )
            
            # è·å–embeddingså¯¹è±¡
            embeddings = memory.get_embeddings()
            
            # æµ‹è¯•æ–‡æœ¬
            test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­ï¼Œç”¨äºéªŒè¯embeddingåŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚"
            
            # ç”Ÿæˆembedding
            embedding_vector = embeddings.embed_query(test_text)
            
            if embedding_vector and len(embedding_vector) > 0:
                print(f"  âœ… Embedding æµ‹è¯•æˆåŠŸ")
                print(f"  ğŸ“Š å‘é‡ç»´åº¦: {len(embedding_vector)}")
                print(f"  ğŸ”¢ å‰5ä¸ªå€¼: {embedding_vector[:5]}")
                return True, f"å‘é‡ç»´åº¦: {len(embedding_vector)}"
            else:
                print(f"  âŒ Embedding è¿”å›ç©ºå‘é‡")
                return False, "ç©ºå‘é‡"
                
        except Exception as e:
            error_msg = str(e)
            print(f"  âŒ Embedding æµ‹è¯•å¤±è´¥: {error_msg}")
            return False, error_msg

    async def test_all_models(self):
        """æµ‹è¯•æ‰€æœ‰æ¨¡å‹"""
        print("ğŸš€ å¼€å§‹æµ‹è¯•æ‰€æœ‰æ¨¡å‹...")
        print()
        
        # æµ‹è¯•LLMæ¨¡å‹
        llm_tests = [
            ("Fast LLM", self.config.fast_llm_provider, self.config.fast_llm_model, self.config.fast_token_limit),
            ("Smart LLM", self.config.smart_llm_provider, self.config.smart_llm_model, self.config.smart_token_limit),
            ("Strategic LLM", self.config.strategic_llm_provider, self.config.strategic_llm_model, self.config.strategic_token_limit),
        ]
        
        for model_name, provider, model, token_limit in llm_tests:
            success, result = await self.test_llm_model(model_name, provider, model, token_limit)
            self.results[model_name] = {
                "success": success,
                "result": result,
                "config": f"{provider}:{model}"
            }
            print()  # ç©ºè¡Œåˆ†éš”
        
        # æµ‹è¯•Embeddingæ¨¡å‹
        success, result = await self.test_embedding_model()
        self.results["Embedding"] = {
            "success": success,
            "result": result,
            "config": f"{self.config.embedding_provider}:{self.config.embedding_model}"
        }
        print()

    def print_summary(self):
        """æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦"""
        print("=" * 60)
        print("ğŸ“‹ æµ‹è¯•ç»“æœæ‘˜è¦")
        print("=" * 60)
        
        success_count = 0
        total_count = len(self.results)
        
        for model_name, result in self.results.items():
            status = "âœ… æˆåŠŸ" if result["success"] else "âŒ å¤±è´¥"
            print(f"{model_name:15} | {status:8} | {result['config']}")
            if result["success"]:
                success_count += 1
            else:
                print(f"                  é”™è¯¯: {result['result']}")
        
        print("-" * 60)
        print(f"æ€»è®¡: {success_count}/{total_count} ä¸ªæ¨¡å‹å¯ç”¨")
        
        if success_count == total_count:
            print("ğŸ‰ æ‰€æœ‰æ¨¡å‹éƒ½å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼")
        elif success_count > 0:
            print("âš ï¸  éƒ¨åˆ†æ¨¡å‹å¯ç”¨ï¼Œè¯·æ£€æŸ¥å¤±è´¥çš„æ¨¡å‹é…ç½®")
        else:
            print("ğŸš¨ æ‰€æœ‰æ¨¡å‹éƒ½æ— æ³•ä½¿ç”¨ï¼Œè¯·æ£€æŸ¥APIé…ç½®")
        
        print("=" * 60)

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” GPT-Researcher é€†å‘APIæ¨¡å‹æµ‹è¯•å·¥å…·")
    print()
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = ModelTester()
        
        # æ‰“å°é…ç½®ä¿¡æ¯
        tester.print_config_info()
        
        # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
        await tester.test_all_models()
        
        # æ‰“å°æ‘˜è¦
        tester.print_summary()
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main())
