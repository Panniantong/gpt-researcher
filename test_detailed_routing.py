#!/usr/bin/env python3
"""
è¯¦ç»†æµ‹è¯•ä¸åŒæ¨¡å‹çš„APIè·¯ç”±
"""

from dotenv import load_dotenv
import os
import asyncio

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

from gpt_researcher.utils.llm import create_chat_completion

async def test_model_routing(model_name, expected_api):
    """æµ‹è¯•ç‰¹å®šæ¨¡å‹çš„è·¯ç”±"""
    print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹: {model_name}")
    print(f"é¢„æœŸä½¿ç”¨: {expected_api}")
    
    test_messages = [
        {"role": "user", "content": "è¯·ç®€å•å›å¤'æµ‹è¯•æˆåŠŸ'"}
    ]
    
    try:
        response = await create_chat_completion(
            model=model_name,
            messages=test_messages,
            temperature=0.1,
            max_tokens=50,
            llm_provider="openai",
            stream=False,
            websocket=None
        )
        
        if "APIè°ƒç”¨å¤±è´¥" in response or "å“åº”ç”Ÿæˆå¤±è´¥" in response:
            print(f"âŒ æ¨¡å‹ {model_name} è°ƒç”¨å¤±è´¥")
            print(f"é”™è¯¯å“åº”: {response[:100]}...")
        else:
            print(f"âœ… æ¨¡å‹ {model_name} è°ƒç”¨æˆåŠŸ")
            print(f"å“åº”: {response[:50]}...")
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹ {model_name} å‡ºç°å¼‚å¸¸: {str(e)}")

async def main():
    print("=== è¯¦ç»†APIè·¯ç”±æµ‹è¯• ===")
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„æ¨¡å‹
    test_cases = [
        ("o3-mini", "å®˜æ–¹OpenAI API"),
        ("o3", "å®˜æ–¹OpenAI API"), 
        ("o3-2025-04-16", "å®˜æ–¹OpenAI API"),
        ("gpt-4o-mini", "é€†å‘API"),
        ("gpt-4", "é€†å‘API"),
        ("gpt-3.5-turbo", "é€†å‘API")
    ]
    
    for model, expected_api in test_cases:
        await test_model_routing(model, expected_api)
        await asyncio.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

    print("\n=== æ£€æŸ¥å½“å‰é¡¹ç›®é…ç½® ===")
    from gpt_researcher.config.config import Config
    cfg = Config()
    print(f"å½“å‰SMART_LLMé…ç½®: {cfg.smart_llm}")
    print(f"è§£æåçš„æ¨¡å‹: {cfg.smart_llm_model}")
    print(f"è§£æåçš„æä¾›å•†: {cfg.smart_llm_provider}")

if __name__ == "__main__":
    asyncio.run(main())